{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"width:80%;\" src=\"images/header.png\" alt=\"Python\"/>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by <a href=\"mailto:german.priks@nhs.net\">German Priks</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will look at how you can use Python, a generalist's programming language with a strong data science pedigree, to answer Information Requests, FoIs, and other data queries.\n",
    "\n",
    "Before diving in, it's worth spending a moment on the question, why use Python for this set of tasks in the first place. Without having a compelling answer to this question, the \"how\" part becomes mostly irrelevant. We already have `SPSS`, `SQL`, `Excel VBA` and `R` to contend with so why another tool / language?\n",
    "\n",
    "Here are the main advantages of Python that come to my mind after having worked with it in data-related contexts for close to 4 years:\n",
    "***\n",
    " - <h4>Python is good <i>to</i> beginners</h4>\n",
    "\n",
    "It's hard to get off the ground with a new language. In the beginning, there are many potential stumbling blocks: from the initial setup and unfamiliar error messages to the idiosyncratic syntax that appears to have more in common with machine code than a human-readable interface. Python is not completely immune to these, but its designers have made a concerted effort to make things simple. To quote [Zen of Python](https://www.python.org/dev/peps/pep-0020/), Simple is better than Complex; Complex is better than Complicated.\n",
    "***\n",
    " - <h4>Python is fast <i>enough</i></h4>\n",
    "    \n",
    "Fast enough to quickly iterate on a new idea, fast enough to crunch through GBs of data in seconds, fast enough for _most_ production-ready projects. And when your Python code is already as performant as it can possibly be, and you still need more speed, Python makes it easy to compile into [faster, closer-to-the-metal languages](https://www.youtube.com/watch?v=x58W9A2lnQc) or [run chunks of code in parallel](https://dask.org/). \n",
    "***\n",
    " - <h4>Python has a mature data science ecosystem</h4>\n",
    "\n",
    "No programming language is an island. There are libraries, extensions, IDEs that grow around a language as the needs of its users evolve. Python is no exception. One of the strenghts of Python's data science (DS) stack is that the community over the years has aligned on a few key libraries to provide the baseline functionality that meets 95% of user DS needs. From the quality point of view, this concentrates developer effort and expertise and from the user point of view, it means you only need to learn the conventions and APIs of one or two libraries. Contrast that with `Javascript` and its framework wars. \n",
    "*** \n",
    " - <h4>Python is open source and has a great community</h4>\n",
    "    \n",
    "Python is not a niche language: it's used extensively by commercial giants, like Netflix and [AstraZeneca](https://stxnext.com/blog/2020/03/17/most-interesting-companies-using-python/), machine learning start-ups, web developers, IoT hobbyists and many others. It's been consistenly voted to be among top [most loved programming languages](https://insights.stackoverflow.com/survey/2019#technology-_-most-loved-dreaded-and-wanted-languages) by StackOverflow developers and is the fastest growing language. Thanks to this community, getting help when starting out with Python is easy, and when you want to expand your knowledge beyond basic scripting, there is a wealth of advanced tutorials online and in printed form.  \n",
    "***\n",
    "\n",
    "### Let's get started\n",
    "Moving on now to the main body of the tutorial, which is to show how Python can be used to answer common IR queries using SMR datamarts. I'm not going into detail of how to setup Python on your machine - a complete guide is available [here](https://nbviewer.jupyter.org/github/Health-SocialCare-Scotland/Python-Resources/blob/master/Python%20Guidance%20for%20PHI.ipynb?flush_cache=true) - but it basically goes like this: ask NSS IT to install Anaconda on your machine, start coding. To connect to SMR datasets, you would also need an Oracle client: `Oracle Client (64bit) 12.2.0c` from NSS IT.\n",
    "\n",
    "The tutorial is written in a Jupyter notebook which lets you combine HTML elements (Markdown), in-line graphics and cells with code snippets. It's part of the standard Anaconda distribution and is a feature-rich interactive development environment.\n",
    "***\n",
    "\n",
    "## Police requests\n",
    "\n",
    "Let's image that Paddington Bear has gone missing and the police are looking for him. They ask us to check if Paddington has had any contacts with the health service in Scotland. So it's our job to run a search for the itinerant bear in our SMR datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First step in any Python code is to import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd           #pandas is the main data-processing library in Python\n",
    "import pyodbc                 #pyodbc is a library to connect to ODBC databases\n",
    "\n",
    "from getpass import getpass   #optional standard library import for hiding login details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we connect to SMRA using our login & password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Login ········\n",
      "Password ··········\n"
     ]
    }
   ],
   "source": [
    "login = getpass(\"Login\")\n",
    "password = getpass(\"Password\")\n",
    "cnxn = pyodbc.connect(f\"DSN=SMRA; UID={login}; PWD={password}\") #f-string enables the injection of variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write SQL to search for Paddington Bear hits in SMR01 using a special Paddington CHI number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triple quotes allow us to write multi-line strings\n",
    "sql = \"\"\"\n",
    "SELECT FIRST_FORENAME, SURNAME, PREVIOUS_SURNAME, CI_CHI_NUMBER, LINK_NO, DOB, ADMISSION_DATE\n",
    "FROM ANALYSIS.SMR01_PI\n",
    "WHERE CI_CHI_NUMBER = '7233464866'\n",
    "OR UPI_NUMBER = 7233464866\n",
    "OR (FIRST_FORENAME = 'PADDINGTON' AND SURNAME = 'BEAR'\n",
    "AND DOB = to_date('1958-10-13', 'yyyy-MM-dd'))\n",
    "AND ADMISSION_DATE >= to_date('2020-01-01', 'yyyy-MM-dd')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Pandas to run our SQL query and fetch any rows that it returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df is shorthand for DataFrame which is the standard data storage unit in Pandas\n",
    "# you can explictly specify function parameter names or rely on positional order\n",
    "df = pd.read_sql(sql=sql, con=cnxn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIRST_FORENAME</th>\n",
       "      <th>SURNAME</th>\n",
       "      <th>PREVIOUS_SURNAME</th>\n",
       "      <th>CI_CHI_NUMBER</th>\n",
       "      <th>LINK_NO</th>\n",
       "      <th>DOB</th>\n",
       "      <th>ADMISSION_DATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [FIRST_FORENAME, SURNAME, PREVIOUS_SURNAME, CI_CHI_NUMBER, LINK_NO, DOB, ADMISSION_DATE]\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no rows returned\n",
    "df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond basic analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Police requests are fairly standard pieces of analysis and SQL for them can be easily written on an ad-hoc basis by hand. However, for more advanced queries, it is often easier to write SQL in a separate word editor with SQL syntax highlighting and then import it to Python (or R, for that matter). One such editor is Notepad ++ with [SQLInForm](https://www.sqlinform.com/free-notepad-plugin/) plugin. Visual Studio Code also has a number of free extensions that enable SQL syntax formatting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's say we wanted to find out the following:\n",
    ">Number of outpatient episodes in NHS Forth Valley between two dates broken down by financial year, clinic and referral types.\n",
    "\n",
    "#### After applying auto-formatting your SQL might look like this:\n",
    "\n",
    "<img align=\"left\" src=\"images/outpatients_demo_sql.png\" alt=\"Demo SQL\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assuming the SQL query is saved in a file called `demo_sql.txt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in and re-format SQL from NPP++\n",
    "with open(\"data/demo_sql.txt\", \"r\") as f:    # open the file in a reading mode\n",
    "    sql =  \"\".join(f.readlines())            # join each line of text, removing line breaks\n",
    "    sql = \" \".join(sql.split())              # split and re-join using a single whitespace to remove indents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"WITH T AS ( SELECT CASE WHEN extract(month FROM CLINIC_DATE) > 3 THEN extract(year FROM CLINIC_DATE) ELSE extract(year FROM CLINIC_DATE) -1 END as FIN_YEAR , CLINIC_ATTENDANCE , HBTREAT_CURRENTDATE, CLINIC_TYPE , REFERRAL_SOURCE , REFERRAL_TYPE FROM ANALYSIS.SMR00_PI WHERE HBTREAT_CURRENTDATE = 'S08000019' AND CLINIC_DATE >= to_date('2014-01-01', 'yyyy-MM-dd') AND CLINIC_DATE <= to_date('2018-10-01', 'yyyy-MM-dd') ) SELECT FIN_YEAR , HBTREAT_CURRENTDATE, CLINIC_ATTENDANCE , CLINIC_TYPE , REFERRAL_SOURCE , REFERRAL_TYPE , count(*) as Total_Episodes FROM T GROUP BY FIN_YEAR , HBTREAT_CURRENTDATE, CLINIC_ATTENDANCE , CLINIC_TYPE , REFERRAL_SOURCE , REFERRAL_TYPE\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries need to be imported only once; we can also re-use our connection to SMRA\n",
    "df = pd.read_sql(sql=sql, con=cnxn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(603, 7)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape attribute stores information about the number of rows and column is the dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some of the advantages of pushing these simple aggregations and selections into SQL include:\n",
    " - __Information governance__ - confidential episode level data doesn't leave the datamart.\n",
    " - __Network / server load__: transferring large, disaggregated files impacts capacity.\n",
    " - __Speed__: running SQL queries is often faster than transferring disaggregated episodes and aggregating them locally\n",
    " \n",
    "I've used episodes in my example, but you can aggregate your data into Continuous Inpatient Spells as well, using stardard syntax appropriate to your team's analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where's Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we've only seen the barest minimum of Python code, using it as an interface to send SQL queries to our Oracle datamart and fetch results.\n",
    "\n",
    "What follows is a collection of code snippets and recipes that you might find useful to do more fine-grained / iterative analysis that is impractical or impossible to write in SQL. The list is by no means complete, and if you think a particularly common piece of analysis is missing, please open a pull request and I'll add it.\n",
    "\n",
    "These examples assume you have a dataframe with raw data saved in a variable called `df` as per Pandas convention. \n",
    "\n",
    "The notebook itself doesn't have any embedded / saved data so running the cells on their own will result in an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output frequency of values in the Admission Type column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ADMISSION_TYPE\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if column URI has any duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.URI.duplicated().any() # note that you can use dot notation to reference columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace Health Board codes with names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hb_dict = {\n",
    "    'S08000015':'NHS Ayrshire and Arran',\n",
    "    'S08000016':'NHS Borders',\n",
    "    'S08000017':'NHS Dumfries and Galloway',\n",
    "    'S08000018':'NHS Fife',\n",
    "    'S08000019':'NHS Forth Valley',\n",
    "    'S08000020':'NHS Grampian',\n",
    "    'S08000021':'NHS Greater Glasgow and Clyde',\n",
    "    'S08000022':'NHS Highland',\n",
    "    'S08000023':'NHS Lanarkshire',\n",
    "    'S08000024':'NHS Lothian',\n",
    "    'S08000025':'NHS Orkney',\n",
    "    'S08000026':'NHS Shetland',\n",
    "    'S08000027':'NHS Tayside',\n",
    "    'S08000028':'NHS Western Isles',\n",
    "    'S08100001':'Golden Jubilee National Hospital',\n",
    "    'S27000001':'Non-NHS Provider',\n",
    "    'S08200001':'England/Wales/Northern Ireland',\n",
    "    'S08200002':'No Fixed Abode',\n",
    "    'S08200003':'Not Known',\n",
    "    'S08200004':'Outside U.K.',\n",
    "}\n",
    "\n",
    "# map method will map values (keys in the dictionary) in the given column (series) to the values\n",
    "# from the supplied dictionary. If value can't be matched, it return a NaN which we fill with\n",
    "# \"other\". Check what codes went into Other just to be sure you're not missing anything.\n",
    "\n",
    "df['HB_DESC'] = df['HBRES_CURRENTDATE'].map(hb_dict).fillna('Other')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DATE'] = pd.to_datetime(df.DATE_MY, format='%d/%m/%Y')\n",
    "\n",
    "df['DISCHARGE_YEAR'] = pd.DatetimeIndex(df['DISCHARGE_DATE']).year\n",
    "df['DISCHARGE_MONTH'] = pd.DatetimeIndex(df['DISCHARGE_DATE']).month\n",
    "df['DISCHARGE_MONTH_NAME'] = pd.DatetimeIndex(df['DISCHARGE_DATE']).strftime('%B')\n",
    "\n",
    "df['WEEK_START'] = (df['ADMISSION_DATE'] - \n",
    "                     pd.to_timedelta(df['ADMISSION_DATE'].dt.weekday, 'D'))\n",
    "\n",
    "# offset by six days because of the zero-based index, i.e. Monday is 0, Tuesday is 1.\n",
    "df['WEEK_END'] = (df['ADMISSION_DATE'] +  DateOffset(days=6) - \n",
    "                     pd.to_timedelta(df['ADMISSION_DATE'].dt.weekday, 'D'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating custom age bands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# must include lower and upper limits: last bin is basically the maximum age in the DF\n",
    "age_bands = [0,4,9,14,19,24,29,34,39,44,49,54,59,64,69,74,79,84,89,94,115]\n",
    "\n",
    "labels = ['00-04', '05-09', '10-14', '15-19', '20-24', '25-29', '30-34',\n",
    "          '35-39', '40-44', '45-49', '50-54', '55-59', '60-64', '65-69',\n",
    "          '70-74', '75-79', '80-84', '85-89', '90-94','95+']\n",
    "\n",
    "df['AGE_BANDS'] = pd.cut(df['AGE_IN_YEARS'], age_bands, labels=labels, include_lowest=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating length of stay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LoS'] = (df['DISCHARGE_DATE'] - df['ADMISSION_DATE']).dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate number of patients per year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for multi-line chaining of methods, enclose the code in brackets\n",
    "(df\n",
    " .drop_duplicates(subset=['LINK_NO', 'YEAR_OF_DISCHARGE'], inplace=True)\n",
    " .groupby('YEAR_OF_DISCHARGE').size()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find a string match using regular expressions:\n",
    "\n",
    "Here we're also importing `numpy` which is the second of the two mainstay library of data analysis in Python. It's primarily used for working with arrays and matrices and vectorised calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "cardio_regex = '(I2[1-3])|(I46)|(I50)|(I472)|(I490)|(R570)|(I6[0-9])|(T82)'\n",
    "\n",
    "df['PRIMARY'] = np.where(df['PRIMARY_CAUSE_OF_DEATH'].str.contains(cardio_regex, regex=True, na=False), 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter and sort:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df['INPATIENT_DAYCASE_IDENTIFIER'] == 'D') & (df['ADMISSION'] == 'ELECTIVE')\n",
    "df[mask].sort_values(by='FIN_YEAR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining (appending to end) of two dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_specialties_smr0['INPATIENT_OUTPATIENT_FLAG'] = 'OUTPATIENT'\n",
    "df_specialties_smr1['INPATIENT_OUTPATIENT_FLAG'] = 'INPATIENT'\n",
    "\n",
    "df_specialties = pd.concat([df_specialties_smr1, df_specialties_smr0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group and aggregate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('FIN_YEAR').TOTAL_EPISODES.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group and filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example1: find all LINK_NO groups with more than two rows:\n",
    "(df.groupby('LINK_NO')\n",
    "        .filter(lambda x: len(x) > 2)\n",
    "        .sort_values(['LINK_NO', 'COPD', 'ASTHMA'], ascending=False))\n",
    "\n",
    "# Example2: find all LINK_NO groups that have both COPD and ASTHMA flags in any position in the CIS\n",
    "(df.groupby(['LINK_NO', 'CIS_MARKER'])\n",
    "        .filter(lambda x: \\ #continuation on new line\n",
    "         (x['COPD'].any() == 1) &\n",
    "         (x['ASTHMA'].any() == 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pivot the data for presentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a basic example; you can also have multiple column levels, etc.\n",
    "excel_tab_1 = df.pivot_table(\n",
    "                  index='HB_DESC',\n",
    "                  columns=['YEAR_OF_DISCHARGE', 'AGE_BANDS'],\n",
    "                  values='Number_of_Patients_Main')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export data to .csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covering all aspects of a data analysis workflow is hardly possible in a single tutorial (I haven't even touched on visualisation and that is my favourite subject), but hopefully I've given you enough of an idea about Python that you can feel more confident discussing it and comparing it with other technologies when it comes to data analysis. Below I've included links to a few online resources about Pandas and/or data analysis. You're also very welcome to contact me directly or via [PHS Python User Group](mailto:nss.pythonusergp@nhs.net) if you have any questions or comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Resources and tutorials:__\n",
    "\n",
    "- [10 Minutes to Pandas](https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html)\n",
    "- [Pandas cookbook](https://tutswiki.com/pandas-cookbook/chapter1/)\n",
    "- [Modern Pandas](https://tomaugspurger.github.io/modern-1-intro.html)\n",
    "- [GIS loves Python](https://automating-gis-processes.github.io/2016/)\n",
    "- [Python for healthcare modelling and data science](https://pythonhealthcare.org/)\n",
    "- [Data Carpentry Python Course](https://datacarpentry.org/python-ecology-lesson/)\n",
    "- [Scientific Computing in Python - history](https://www.nature.com/articles/s41592-019-0686-2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data Analysis [conda env:data]",
   "language": "python",
   "name": "conda-env-data-data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
