{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"width:80%;\" src=\"images/header.png\" alt=\"Python\"/>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by <a href=\"mailto:german.priks@nhs.net\">German Priks</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will look at how you can use Python, a generalist's programming language with a strong data science pedigree, to answer Information Requests, FoIs, and other data queries.\n",
    "\n",
    "Before diving in, it's worth spending a moment on the question, why use Python for this set of tasks in the first place. Without having a compelling answer to this question, the \"how\" part becomes mostly irrelevant. We already have `SPSS`, `SQL`, `Excel VBA` and `R` to contend with so why another tool / language?\n",
    "\n",
    "Here are the main advantages of Python that come to my mind after having worked with it in data-related contexts for close to 4 years:\n",
    "***\n",
    " - <h4>Python is good <i>to</i> beginners</h4>\n",
    "\n",
    "It's hard to get off the ground with a new language. In the beginning, there are many potential stumbling blocks: from the initial setup and unfamiliar error messages to the idiosyncratic syntax that appears to have more in common with machine code than a human-readable interface. Python is not completely immune to these, but its designers have made a concerted effort to make things simple. To quote [Zen of Python](https://www.python.org/dev/peps/pep-0020/), Simple is better than Complex; Complex is better than Complicated.\n",
    "***\n",
    " - <h4>Python is fast <i>enough</i></h4>\n",
    "    \n",
    "Fast enough to quickly iterate on a new idea, fast enough to crunch through GBs of data in seconds, fast enough for _most_ production-ready projects. And when your Python code is already as performant as it can possibly be, and you still need more speed, Python makes it easy to compile into [faster, closer-to-the-metal languages](https://www.youtube.com/watch?v=x58W9A2lnQc) or [run chunks of code in parallel](https://dask.org/). \n",
    "***\n",
    " - <h4>Python has a mature data science ecosystem</h4>\n",
    "\n",
    "No programming language is an island. There are libraries, extensions, IDEs that grow around a language as the needs of its users evolve. Python is no exception. One of the strenghts of Python's data science (DS) stack is that the community over the years has aligned on a few key libraries to provide the baseline functionality that meets 95% of user DS needs. From the quality point of view, this concentrates developer effort and expertise and from the user point of view, it means you only need to learn the conventions and APIs of one or two libraries. Contrast that with `Javascript` and its framework wars. \n",
    "*** \n",
    " - <h4>Python is open source and has a great community</h4>\n",
    "    \n",
    "Python is not a niche language: it's used extensively by commercial giants, like Netflix and [AstraZeneca](https://stxnext.com/blog/2020/03/17/most-interesting-companies-using-python/), machine learning start-ups, web developers, IoT hobbyists and many others. It's been consistenly voted to be among top [most loved programming languages](https://insights.stackoverflow.com/survey/2019#technology-_-most-loved-dreaded-and-wanted-languages) by StackOverflow developers and is the fastest growing language. Thanks to this community, getting help when starting out with Python is easy, and when you want to expand your knowledge beyond basic scripting, there is a wealth of advanced tutorials online and in printed form.  \n",
    "***\n",
    "\n",
    "### Let's get started\n",
    "Moving on now to the main body of the tutorial, which is to show how Python can be used to answer common IR queries using SMR datamarts. I'm not going into detail of how to setup Python on your machine - a complete guide is available [here](https://nbviewer.jupyter.org/github/Health-SocialCare-Scotland/Python-Resources/blob/master/Python%20Guidance%20for%20PHI.ipynb?flush_cache=true) - but it basically goes like this: ask NSS IT to install Anaconda on your machine, start coding. \n",
    "\n",
    "The tutorial is written in a Jupyter notebook which lets you combine HTML elements (Markdown), in-line graphics and cells with code snippets. It's part of the standard Anaconda distribution and is a feature-rich interactive development environment.\n",
    "***\n",
    "\n",
    "## Police requests\n",
    "\n",
    "Let's image that Paddington Bear has gone missing and the police are looking for him. They ask us to check if Paddington has had any contacts with the health service in Scotland. So it's our job to run a search for the itinerant bear in our SMR datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First step in any Python code is to import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd           #pandas is the main data-processing library in Python\n",
    "import pyodbc                 #pyodbc is a library to connect to ODBC databases\n",
    "\n",
    "from getpass import getpass   #optional standard library import for hiding login details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we connect to SMRA using our login & password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Login ········\n",
      "Password ··········\n"
     ]
    }
   ],
   "source": [
    "login = getpass(\"Login\")\n",
    "password = getpass(\"Password\")\n",
    "cnxn = pyodbc.connect(f'DSN=SMRA; UID={login}; PWD={password}') #f-string enables the injection of variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write SQL to search for Paddington Bear hits in SMR01 using a special Paddington CHI number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#triple quotes allow us to write multi-line strings\n",
    "sql = \"\"\"\n",
    "SELECT FIRST_FORENAME, SURNAME, PREVIOUS_SURNAME, CI_CHI_NUMBER, LINK_NO, DOB, ADMISSION_DATE\n",
    "FROM ANALYSIS.SMR01_PI\n",
    "WHERE CI_CHI_NUMBER = '7233464866'\n",
    "OR UPI_NUMBER = 7233464866\n",
    "OR (FIRST_FORENAME = 'PADDINGTON' AND SURNAME = 'BEAR'\n",
    "AND DOB = to_date('1958-10-13', 'yyyy-MM-dd'))\n",
    "AND ADMISSION_DATE >= to_date('2020-01-01', 'yyyy-MM-dd')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Pandas to run our SQL query and fetch any rows that it returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df is shorthand for DataFrame which is the standard data storage unit in Pandas\n",
    "df = pd.read_sql(sql=sql, con=cnxn) #you can explictly specify function parameter names or rely on positional order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIRST_FORENAME</th>\n",
       "      <th>SURNAME</th>\n",
       "      <th>PREVIOUS_SURNAME</th>\n",
       "      <th>CI_CHI_NUMBER</th>\n",
       "      <th>LINK_NO</th>\n",
       "      <th>DOB</th>\n",
       "      <th>ADMISSION_DATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [FIRST_FORENAME, SURNAME, PREVIOUS_SURNAME, CI_CHI_NUMBER, LINK_NO, DOB, ADMISSION_DATE]\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df #no rows returned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond basic analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Police requests are fairly standard pieces of analysis and SQL for them can be easily written on an ad-hoc basis by hand. However, for more advanced queries, it is often easier to write SQL in a separate word editor with SQL syntax highlighting and then import it to Python (or R, for that matter). One such editor is Notepad ++ with [SQLInForm](https://www.sqlinform.com/free-notepad-plugin/) plugin. Visual Studio Code also has a number of free extensions that enable SQL syntax formatting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's say we wanted to find out the following:\n",
    "Number of outpatient episodes in NHS Forth Valley between two dates broken down by financial year, clinic and referral types.\n",
    "\n",
    "#### After applying auto-formatting your SQL might look like this:\n",
    "\n",
    "<img align=\"left\" src=\"images/outpatients_demo_sql.png\" alt=\"Demo SQL\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assuming the SQL query is saved in a file called `demo_sql.txt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in and re-format SQL from NPP++\n",
    "with open('data/demo_sql.txt', 'r') as f:    # open the file in a reading mode\n",
    "    sql =  \"\".join(f.readlines())            # join each line of text, removing line breaks\n",
    "    sql = \" \".join(sql.split())              # split and re-join using a single whitespace to remove indents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"WITH T AS ( SELECT CASE WHEN extract(month FROM CLINIC_DATE) > 3 THEN extract(year FROM CLINIC_DATE) ELSE extract(year FROM CLINIC_DATE) -1 END as FIN_YEAR , CLINIC_ATTENDANCE , HBTREAT_CURRENTDATE, CLINIC_TYPE , REFERRAL_SOURCE , REFERRAL_TYPE FROM ANALYSIS.SMR00_PI WHERE HBTREAT_CURRENTDATE = 'S08000019' AND CLINIC_DATE >= to_date('2014-01-01', 'yyyy-MM-dd') AND CLINIC_DATE <= to_date('2018-10-01', 'yyyy-MM-dd') ) SELECT FIN_YEAR , HBTREAT_CURRENTDATE, CLINIC_ATTENDANCE , CLINIC_TYPE , REFERRAL_SOURCE , REFERRAL_TYPE , count(*) as Total_Episodes FROM T GROUP BY FIN_YEAR , HBTREAT_CURRENTDATE, CLINIC_ATTENDANCE , CLINIC_TYPE , REFERRAL_SOURCE , REFERRAL_TYPE\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries need to be imported only once; we can also re-use our connection to SMRA\n",
    "df = pd.read_sql(sql=sql, con=cnxn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(603, 7)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the query returned 603 rows and 7 columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some of the advantages of pushing these simple aggregations and selections into SQL include:\n",
    " - __Information governance__ - confidential episode level data doesn't leave the datamart.\n",
    " - __Network / server load__: transferring large, disaggregated files impacts capacity.\n",
    " - __Speed__: running SQL queries is often faster than transferring disaggregated episodes and aggregating them locally\n",
    " \n",
    "I've used episodes in my example, but you can aggregate your data into Continuous Inpatient Spells as well, using stardard syntax appropriate to your team's analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where's Python?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data Analysis [conda env:data]",
   "language": "python",
   "name": "conda-env-data-data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
